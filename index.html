<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 5.2.0">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">


<link rel="stylesheet" href="/lib/font-awesome/css/all.min.css">

<script id="hexo-configurations">
    var NexT = window.NexT || {};
    var CONFIG = {"hostname":"example.com","root":"/","scheme":"Gemini","version":"7.8.0","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12,"onmobile":false},"copycode":{"enable":false,"show_result":false,"style":null},"back2top":{"enable":true,"sidebar":false,"scrollpercent":false},"bookmark":{"enable":false,"color":"#222","save":"auto"},"fancybox":false,"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"algolia":{"hits":{"per_page":10},"labels":{"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}},"localsearch":{"enable":false,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false},"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}}};
  </script>

  <meta property="og:type" content="website">
<meta property="og:title" content="Dong">
<meta property="og:url" content="http://example.com/index.html">
<meta property="og:site_name" content="Dong">
<meta property="og:locale" content="en_US">
<meta property="article:author" content="Dong">
<meta name="twitter:card" content="summary">

<link rel="canonical" href="http://example.com/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome : true,
    isPost : false,
    lang   : 'en'
  };
</script>

  <title>Dong</title>
  






  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

</head>

<body itemscope itemtype="http://schema.org/WebPage">
  <div class="container use-motion">
    <div class="headband"></div>

    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="Toggle navigation bar">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <span class="logo-line-before"><i></i></span>
      <h1 class="site-title">Dong</h1>
      <span class="logo-line-after"><i></i></span>
    </a>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
    </div>
  </div>
</div>




<nav class="site-nav">
  <ul id="menu" class="main-menu menu">
        <li class="menu-item menu-item-home">

    <a href="/" rel="section"><i class="fa fa-home fa-fw"></i>Home</a>

  </li>
        <li class="menu-item menu-item-about">

    <a href="/about/" rel="section"><i class="fa fa-user fa-fw"></i>About</a>

  </li>
        <li class="menu-item menu-item-tags">

    <a href="/tags/" rel="section"><i class="fa fa-tags fa-fw"></i>Tags</a>

  </li>
        <li class="menu-item menu-item-categories">

    <a href="/categories/" rel="section"><i class="fa fa-th fa-fw"></i>Categories</a>

  </li>
        <li class="menu-item menu-item-archives">

    <a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>Archives</a>

  </li>
  </ul>
</nav>




</div>
    </header>

    
  <div class="back-to-top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>


    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          

          <div class="content index posts-expand">
            
      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="http://example.com/2020/11/11/content/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Dong">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Dong">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2020/11/11/content/" class="post-title-link" itemprop="url">CONTENTS</a>
        </h2>

        <div class="post-meta">
          
          
              <i class="fa fa-thumb-tack"></i>
              <font color=red>TOP</font>
              <span class="post-meta-divider">|</span>
          
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2020-11-11 12:53:14" itemprop="dateCreated datePublished" datetime="2020-11-11T12:53:14+08:00">2020-11-11</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2021-04-07 10:18:45" itemprop="dateModified" datetime="2021-04-07T10:18:45+08:00">2021-04-07</time>
              </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h3 id="Blog-categerories"><a href="#Blog-categerories" class="headerlink" title="Blog categerories"></a>Blog categerories</h3><p>01001: CS-Introduction to Statistics<br>01002: CS-Principles of Data Science<br>01003: CS-Database Management System<br>01004: CS-Algorithm<br>01005: CS-ML-others<br>01006: CS-Introduction to ML and DM<br>01007: CS-NLP<br>01008: CS-Computational Statistical Methods<br>01009: CS-Cloud Computing</p>
<p>…<br>TBD</p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="http://example.com/2021/04/07/01006-5-Decision-Trees/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Dong">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Dong">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2021/04/07/01006-5-Decision-Trees/" class="post-title-link" itemprop="url">01006-5-Decision Trees & Ensemble methods</a>
        </h2>

        <div class="post-meta">
          
          
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2021-04-07 10:26:10" itemprop="dateCreated datePublished" datetime="2021-04-07T10:26:10+08:00">2021-04-07</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2021-04-08 17:22:16" itemprop="dateModified" datetime="2021-04-08T17:22:16+08:00">2021-04-08</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/01006/" itemprop="url" rel="index"><span itemprop="name">01006</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h1 id="Decision-Trees"><a href="#Decision-Trees" class="headerlink" title="Decision Trees"></a>Decision Trees</h1><h4 id="Constructing-decision-trees"><a href="#Constructing-decision-trees" class="headerlink" title="Constructing decision trees"></a>Constructing decision trees</h4><p>&emsp;     • Strategy: top-down learning using recursive divide-and-conquer process:<br>&emsp; &emsp;         • First: Select the best attribute for root node and create branch for each possible attribute value<br>&emsp; &emsp;         • Then: Split examples into subsets, one for each branch extending from the node<br>&emsp; &emsp;         • Finally: Repeat recursively for each branch, using only the examples that reach the branch<br>&emsp;     • Stop if all examples have the same class<br>&emsp; &emsp;         • Make a leaf node for this class</p>
<p>Select the best attribute by:<br>&emsp;  <strong>Entropy</strong>:<br>&emsp;  &emsp;  $ H(S)=I(S)=-\sum_i P_i log_2 P_i$</p>
<p>&emsp;  &emsp;  For binary tasks: $H(S)=I(p,(1-p))=-plog_2p-(1-p)log_2(1-p) \in [0,1]$</p>
<p>&emsp;  <strong>Information Gain</strong>:<br>&emsp;  &emsp; Gain=T1-T2, T1 is the entropy of the set of examples S associated with the parent node <em>before</em> the split. T2 is the remaining entropy in S, <em>after</em> S is split  by the attribute.</p>
<p>&emsp;  <strong>Gain ratio</strong>:<br>&emsp; &emsp;    It takes into account the number of branches when choosing an attribute and penalizes highly-branching attributes</p>
<h4 id="Pruning-Decision-Trees"><a href="#Pruning-Decision-Trees" class="headerlink" title="Pruning Decision Trees"></a>Pruning Decision Trees</h4><p>&emsp;      Two main strategies<br>&emsp; &emsp;         • Pre-pruning - stop growing the tree earlier, before it reaches the point where it perfectly classifies the training data<br>&emsp; &emsp;         • Post-pruning – fully grow the tree, allowing it to perfectly cover the training data, and then prune it<br>&emsp; &emsp; &emsp;    • sub-tree replacement : Bottom-up<br>&emsp; &emsp; &emsp;    • sub-tree raising<br>&emsp; &emsp; &emsp;    • converting the tree to rules and then pruning them</p>
<h4 id="Decision-trees-summary"><a href="#Decision-trees-summary" class="headerlink" title="Decision trees - summary"></a>Decision trees - summary</h4><p>&emsp;     • Very popular ML technique<br>&emsp;     • Top-down learning using recursive divide-and-conquer process<br>&emsp;     • Easy to implement<br>&emsp;     • Interpretable<br>&emsp; &emsp;         • The produced tree is easy to visualize and understand by non- experts and clients<br>&emsp; &emsp;         • Interpretability increases the trust in using the machine learning model in practice<br>&emsp;     • Uses pruning to prevent overfitting<br>&emsp;     • Numeric attributes are converted into nominal (binary split)<br>&emsp;     • Selecting the best attribute – information gain, gain ratio, others<br>&emsp;     • Variations: purity can be measured in different ways, e.g. CART uses Gini Index not entropy</p>
<h1 id="Ensemble-methods"><a href="#Ensemble-methods" class="headerlink" title="Ensemble methods"></a>Ensemble methods</h1><h2 id="Motivation-for-creating-ensembles"><a href="#Motivation-for-creating-ensembles" class="headerlink" title="Motivation for creating ensembles"></a>Motivation for creating ensembles</h2><h4 id="When-ensembles-work-well"><a href="#When-ensembles-work-well" class="headerlink" title="When ensembles work well"></a>When ensembles work well</h4><p>&emsp;          • Conditions for an ensemble to perform better than a single classifier:<br>&emsp;  &emsp;                  • The base classifiers should be good enough, i.e. better than a random guessing ($\varepsilon<0.5 $ for binary classifiers)   
&emsp;  &emsp;                  <br/>• The base classifiers are independent of each other<br>&emsp;          • Independence – in practice:<br>&emsp;  &emsp;                  • It is not possible to ensure total independence among the base classifiers<br>&emsp;  &emsp;                  • Good results have been achieved in ensemble methods when the base classifiers are slightly correlated</p>
<h2 id="Ensemble-methods-1"><a href="#Ensemble-methods-1" class="headerlink" title="Ensemble methods"></a>Ensemble methods</h2><p>&emsp;      • Manipulating the <font color=green><strong>training data</strong></font> – creating multiple training sets by resampling the original data according to some sampling distribution and constructing a classifier for each training set (e.g. <em>Bagging</em> and <em>Boosting</em>)<br>&emsp;      • Manipulating the <font color=green><strong>attributes</strong></font> – using a subset of input features (e.g. <em>Random Forest</em> and <em>Random Subspace</em>)<br>&emsp;      • Manipulating the <font color=green><strong>class labels</strong></font> – will not be covered (e.g. error- correcting output coding)<br>&emsp;     • Manipulating the <font color=green><strong>learning algorithm</strong></font> – e.g. building a set of classifiers with different parameters</p>
<h3 id="Bagging"><a href="#Bagging" class="headerlink" title="Bagging"></a>Bagging</h3><p>&emsp; Bagging is called <strong>Booststrap AGGregatING</strong><br>&emsp;  &emsp; • Given: a dataset D with n example (the original dataset)<br>&emsp;  &emsp; • <span style="border-bottom:1px  dashed red;">Bootstrap sample</span> D’ from D: contains also n examples, randomly chosen from D <span style="border-bottom:1px  dashed red;">with replacement</span> (i.e. some examples from D will appear more than once in D’, some will not appear at all)<br>&emsp; On average, 63% of the examples in D will also appear in D’ as it can be shown that the probability to choose an example is $(1-\frac{1}{n})^n = \frac{1}{e} \approx 0.369$</p>
<p>&emsp; Pseudocode</p>
<blockquote>
<p>&emsp;  <u>Model generation</u><br>&emsp;&emsp;        Let n be the number of examples in the training data<br>&emsp;&emsp;        For each of M iterations:<br>&emsp;&emsp;&emsp;                Sample n examples with replacement from training data<br>&emsp;&emsp;&emsp;                Apply the learning algorithm to the sample<br>&emsp;&emsp;&emsp;                Store the resulting model<br>&emsp;    <u>Classification</u><br>&emsp;&emsp;        For each of the M models:<br>&emsp;&emsp;&emsp;            Predict class of testing example using model<br>&emsp;    Return class that has been predicted most often (majority vote)</p>
</blockquote>
<h3 id="Boosting"><a href="#Boosting" class="headerlink" title="Boosting"></a>Boosting</h3><h4 id="AdaBoost"><a href="#AdaBoost" class="headerlink" title="AdaBoost"></a>AdaBoost</h4><p>AdaBoost algorithm</p>
<blockquote>
<p>   D – given training set, m – number of training examples, M – number of models<br><u>Model generation</u><br>Assign equal weight $p_i$ to each training example i, e.g. $\frac{1}{m}$<br><br/> $p_i$ determines the probability of i to be selected in the training set for the next model<br><br/> For k=1 to M iterations: <font color=gray>//building M models</font><br>&emsp;             Create data subset $D_k$ from D with <em>m</em> ex. by sampling with replacement using p<br>&emsp;             Apply learning algorithm to $D_k$ and store resulting model<br><br/> &emsp;  Compute error $e_i$ of model on each training example i:<br><br/> &emsp; &emsp;              $e_i= 0$ if correctly classified, $e_i =1$ if incorrectly classified<br><br/> &emsp;   Calculate the weighed error of the model $e =sum(p_i *e_i$) over all m examples<br><br/> &emsp;  Update the weights:<br>&emsp; &emsp;               If example classified correctly by model, multiply its weight by $\frac{e}{1-e}$<br><br/> &emsp; &emsp;   Normalize weights (probabilities) of all examples so that they sum to 1<br>k=k+1<br><u>Classification</u><br>For each of the M models:<br>&emsp;       Predict class of testing example using model<br>Combine predictions using weighed vote, where the weight of each model depends on its accuracy on training set used to build the model</p>
</blockquote>
<h4 id="Gradient-Boosting"><a href="#Gradient-Boosting" class="headerlink" title="Gradient Boosting"></a>Gradient Boosting</h4><p>example<br>&emsp;         Create model 1: DT1 fit on training data (X,y), store model<br>&emsp;         To create model 2:<br>&emsp; &emsp;                 • Evaluate DT1 on training data, calculate error: y2 = y (actual value) - predicted value by DT1<br>&emsp; &emsp;                 • Create model 2: DT2 fit on (X,y2), store model<br>&emsp;        To create model 3:<br>&emsp; &emsp;                 • Evaluate DT2 on training data, calculate error: y3 = y2 - predicted value by DT2<br>&emsp; &emsp;                 • Create model 3: DT3 fit on (X,y3), store model<br>&emsp;         Now we have 3 decision trees. To make a prediction for a new example: sum the predictions of DT1, DT2 and DT3</p>
<h4 id="comparison"><a href="#comparison" class="headerlink" title="comparison"></a>comparison</h4><p>&emsp;       Similarities<br>&emsp;  &emsp;                  • Use voting (for classification) and averaging (for prediction) to combine the outputs of the individual learners<br>&emsp;  &emsp;                  • Combine classifiers of the same type, typically trees – e.g. decision stumps or decision trees<br>&emsp;          Differences<br>&emsp;  &emsp;                  • Creating base classifiers:<br>&emsp;  &emsp;  &emsp;                          Bagging – separately<br>&emsp;  &emsp;  &emsp;                          Boosting – iteratively – the new ones are encouraged to become experts for the misclassified examples by the previous base learners (complementary expertise)<br>&emsp;  &emsp;                 • Combination method<br>&emsp;  &emsp;  &emsp;                          Bagging – equal weighs to all base learners<br>&emsp;  &emsp;  &emsp;                          Boosting – different weights - based on performance on training data</p>
<h3 id="Random-Forest"><a href="#Random-Forest" class="headerlink" title="Random Forest"></a>Random Forest</h3><p>Random Forest algorithm</p>
<blockquote>
<p>n - number of training examples, m – number of all features, k – number of features to be used by each ensemble member (k &lt; m), M – number of ensemble members<br><u>Model generation</u><br>For each of M iteration:<br>&emsp;         Bagging – generate a bootstrap sample<br>&emsp;        Sample n instances with replacement from training data<br>&emsp;        Random feature selection for selecting the best attribute<br>&emsp;       Grow decision tree without pruning. At each step select the best feature to split on by considering <u>only k</u> randomly selected features and calculating information gain<br><u>Classification:</u><br>Apply the new example to each of the t decision trees starting from the root. Assign it to the class corresponding to the leaf. Combine the decisions of the individual trees by majority voting.</p>
</blockquote>
<h5 id="Random-Forests-Discussion"><a href="#Random-Forests-Discussion" class="headerlink" title="Random Forests Discussion"></a>Random Forests Discussion</h5><p>&emsp;     • Performance depends on<br>&emsp; &emsp;         • Accuracy of the individual trees (strength of the trees)<br>&emsp; &emsp;         • Correlation between the trees<br>&emsp;     • Ideally: accurate individual trees but less correlated<br>&emsp;     • Bagging and random feature selection are used to generate diversity and reduce the correlation between the trees<br>&emsp;     • As the number of features k increases, both the strength and correlation increase<br>&emsp;     • Random Forest typically outperforms a single decision tree<br>&emsp;     • Robust to overfitting<br>&emsp;     • Fast as only a subset of the features are considered<br>&emsp; </p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="http://example.com/2021/04/07/01006-4-Naive-Bayes/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Dong">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Dong">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2021/04/07/01006-4-Naive-Bayes/" class="post-title-link" itemprop="url">01006-4-Naïve Bayes</a>
        </h2>

        <div class="post-meta">
          
          
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>
              

              <time title="Created: 2021-04-07 10:25:50 / Modified: 23:12:46" itemprop="dateCreated datePublished" datetime="2021-04-07T10:25:50+08:00">2021-04-07</time>
            </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/01006/" itemprop="url" rel="index"><span itemprop="name">01006</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h1 id="Evaluating-Machine-Learning-Methods"><a href="#Evaluating-Machine-Learning-Methods" class="headerlink" title="Evaluating Machine Learning Methods"></a>Evaluating Machine Learning Methods</h1><h2 id="Bayes-theorem"><a href="#Bayes-theorem" class="headerlink" title="Bayes theorem"></a>Bayes theorem</h2><p>Given a hypothesis H and evidence E for this hypothesis, then the probability of H given E is: <script type="math/tex">P(H|E) = \frac {P(E|H)P(H)}{P(E)}</script><br>&emsp; P(H|E): posteriori probability or conditional probability<br>&emsp; P(H): prior probability</p>
<h2 id="Naive-Bayes-algorithm"><a href="#Naive-Bayes-algorithm" class="headerlink" title="Naïve Bayes algorithm"></a>Naïve Bayes algorithm</h2><p>Two assumptions:<br>&emsp;  1) Independence – (the values of the) attributes are conditionally independent of each other, given the class (i.e. for each class value)<br>&emsp;  2) Equally importance – all attributes are equally important</p>
<p>The “zero-frequency” problem:<br>&emsp;  an attribute value does not occur with every class value =&gt; P(E1|H) = 0<br>&emsp; Remedy: add 1 to the numerator and m to the denominator</p>
<p>For missing values:<br>&emsp;        During classification:<br>&emsp;&emsp;                do not include this attribute<br>&emsp;        During training:<br>&emsp;&emsp;            do not include the missing values in the counts<br>&emsp;&emsp;            calculate the probabilities based on the actual number of training examples without missing values for each attribute</p>
<p><strong>For Numeric  Attributes</strong>:<br>&emsp; Assume that the numeric attributes follow a <em>normal</em> (or Gaussian) distribution and use <em>probability density function</em></p>
<script type="math/tex; mode=display">P(E=x|H) = \frac{1}{\sigma\sqrt{2\pi}} e ^{ -\frac{(x-\mu)^2}{2\sigma^2} }</script><h4 id="Naive-Bayes-discussion"><a href="#Naive-Bayes-discussion" class="headerlink" title="Naïve Bayes - discussion"></a>Naïve Bayes - discussion</h4><p>&emsp;      • Probabilities are calculated easily due to the independence assumption<br>&emsp;      • Fast - requires 1 scan of the training data to calculate all statistics for both nominal and continuous attributes<br>&emsp;      • In many cases outperforms more sophisticated learning methods<br>&emsp;      • Robust to isolated noise points – such points have only negligible impact on the conditional probabilities<br>&emsp;      • Correlated attributes reduce the power of Naïve Bayes - violation of the independence assumption<br>&emsp; &emsp;      • Solution: apply feature selection beforehand to identify and discard correlated (redundant) attributes<br>&emsp;      • Normal distribution assumption for numeric attributes - many features are not normally distributed – solutions:<br>&emsp; &emsp;      • Discretize the data first, i.e. numerical -&gt; nominal attributes<br>&emsp; &emsp;     • Use other probability density functions, e.g. Poisson, binomial, gamma</p>
<h2 id="Evaluating-ML-models"><a href="#Evaluating-ML-models" class="headerlink" title="Evaluating ML models"></a>Evaluating ML models</h2><h3 id="Evaluation-procedures"><a href="#Evaluation-procedures" class="headerlink" title="Evaluation procedures"></a>Evaluation procedures</h3><h4 id="Holdout-method"><a href="#Holdout-method" class="headerlink" title="Holdout method"></a>Holdout method</h4><p>&emsp;      • Split the data randomly into 2 sets: training set and test set<br>&emsp; &emsp;   • typically 2/3 and 1/3<br>&emsp;      • Build the model using the training data<br>&emsp;      • Evaluate the model on the test data<br>&emsp; &emsp;   • calculate accuracy or other performance measures</p>
<p> Sometimes we need to use a third set: <strong>validation set</strong><br> &emsp;      • The data is split into: training, validation and test set<br> &emsp;      • For example, some classification methods (decision trees, neural networks) operate in two stages:<br> &emsp;  &emsp;              • Stage 1: Build the classifier<br> &emsp;  &emsp;              • Stage 2: Tune its hyperparameters<br> &emsp;      • The test data can not be used for hyperparameter tuning<br> &emsp;      • Proper evaluation procedure - 3 datasets:<br> &emsp;  &emsp;              • 1) Training set - to build the classifier<br> &emsp;  &emsp;              • 2) Validation set - to tune its hyperparameters<br> &emsp;  &emsp;              • 3) Test set - to evaluate accuracy</p>
<p>Impovements:<br>&emsp;     <strong>Stratification</strong>:<br>&emsp;  &emsp;             Ensures that each class is represented with approximately equal proportions in both data sets (training and testing)<br>&emsp;     <strong>Repeated holdout method</strong><br>&emsp;  &emsp;      The holdout method can be made more reliable by repeating the random split into training and test set several times and calculating average accuracy</p>
<h4 id="Cross-validation"><a href="#Cross-validation" class="headerlink" title="Cross validation"></a>Cross validation</h4><p>&emsp;      10-fold cross-validation – typically used<br>&emsp; &emsp;       Step 1: Split data into 10 sets set1,.., set10 of approximately equal size<br>&emsp; &emsp;       Step 2: A classifier is built 10 times. Each time the testing is on 1 set and the training is on the remaining 9 sets together<br>&emsp; &emsp;       Step 3: Calculate the cross validation accuracy = average (acc1, acc2,…acc10)</p>
<p>10-fold cross-validation =&gt; Stratified 10-fold cross-validation =&gt; repeated stratified 10-fold cross-validation</p>
<h4 id="Leave-one-out-cross-validation"><a href="#Leave-one-out-cross-validation" class="headerlink" title="Leave-one-out cross validation"></a>Leave-one-out cross validation</h4><p>&emsp;      • A special form of n-fold cross-validation<br>&emsp;  &emsp;          • Set the number of folds to the number of training examples<br>&emsp;  &emsp;          • =&gt; for n training examples, build classifier n times<br>&emsp;      • Advantages:<br>&emsp;  &emsp;          • Makes the best use of data - the greatest possible amount of data is used for training<br>&emsp;          • Deterministic procedure – no random sampling is involved - the same result will be obtained every time<br>&emsp;      • Disadvantage<br>&emsp;  &emsp;          • High computational cost, especially for large datasets</p>
<h4 id="Cross-validation-for-parameter-tuning"><a href="#Cross-validation-for-parameter-tuning" class="headerlink" title="Cross-validation for parameter tuning"></a>Cross-validation for parameter tuning</h4><p>&emsp;  grid-search with cross-validation for parameter tuning</p>
<blockquote>
<p>&emsp;    Create the parameter grid (i.e. the parameter combinations)<br>&emsp;      Split the data into training set and test set<br>&emsp;      For each parameter combination:<br>&emsp; &emsp;         Train a k-NN classifier on the training data using 10-fold cross-validation<br>&emsp; &emsp;         Compute the cross-validation accuracy cv_acc<br>&emsp; &emsp;         If cv_acc &gt; best_cv_acc<br>&emsp; &emsp; &emsp;                 best_cv_acc = cv_acc<br>&emsp; &emsp; &emsp;                 best_parameters = current parameters<br>&emsp;          Rebuild the k-NN model using the whole training data and best_parameters<br>&emsp;          Evaluate it on the test data and report the results</p>
</blockquote>
<h3 id="Performance-measures"><a href="#Performance-measures" class="headerlink" title="Performance measures"></a>Performance measures</h3><h4 id="Confusion-matrix"><a href="#Confusion-matrix" class="headerlink" title="Confusion matrix"></a>Confusion matrix</h4><div class="table-container">
<table>
<thead>
<tr>
<th>examples</th>
<th>assigned to class YES</th>
<th>assigned to class YES</th>
</tr>
</thead>
<tbody>
<tr>
<td>class YES</td>
<td>TP</td>
<td>FN</td>
</tr>
<tr>
<td>class NO</td>
<td>FP</td>
<td>TN</td>
</tr>
</tbody>
</table>
</div>
<p>$Acc=\frac{TP+TN}{TP+FN+FP+TN}$,&emsp;    &emsp;<br>$Precision ( P) =\frac{TP}{TP+FP}$,&emsp;    &emsp;<br>$Recall  (R)=\frac{TP}{TP+FN}$,&emsp;    &emsp;<br>$F1=\frac{2PR}{P+R}$</p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="http://example.com/2021/04/07/01006-3-Logistic-Regression/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Dong">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Dong">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2021/04/07/01006-3-Logistic-Regression/" class="post-title-link" itemprop="url">01006-3-Logistic Regression</a>
        </h2>

        <div class="post-meta">
          
          
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>
              

              <time title="Created: 2021-04-07 10:25:29 / Modified: 22:26:18" itemprop="dateCreated datePublished" datetime="2021-04-07T10:25:29+08:00">2021-04-07</time>
            </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/01006/" itemprop="url" rel="index"><span itemprop="name">01006</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h1 id="Linear-Regression-Logistic-Regression-Overfitting-and-Regularization"><a href="#Linear-Regression-Logistic-Regression-Overfitting-and-Regularization" class="headerlink" title="Linear Regression. Logistic Regression. Overfitting and Regularization."></a>Linear Regression. Logistic Regression. Overfitting and Regularization.</h1><h2 id="Linear-Regression"><a href="#Linear-Regression" class="headerlink" title="Linear Regression"></a>Linear Regression</h2><p>&emsp; • Given:      a dataset with 2 continuous variables:<br>&emsp; &emsp;           • feature x (also called independent variable)<br>&emsp; &emsp;           • predicted variable y (also called target variable or dependent variable)<br>&emsp;      Goal: Approximate the relationship between these variables with a straight line for the given dataset<br>&emsp; &emsp;     • Prediction (typical task in DM): Given a new value of independent variable, use the line to predict the value of the dependent variable<br>&emsp; &emsp;    • Descriptive analysis (typical task in psychology, health and social sciences): assess the strength of the relationship between x and y</p>
<p>model:<br>&emsp; &emsp;    $\hat y=b_0+ b_1x$</p>
<p>&emsp; &emsp; Goal : minimize $SSE=\sum_i (y_i - \hat{y_i})^2$</p>
<p>By the least squares method:<br>&emsp; &emsp;  $b_1=\frac{\sum(x_i y_i - \mu(x)\mu(y )}{\sum(x_i^2-\mu(x)^2)}$<br>&emsp; &emsp;  $b_0=\mu(y)-b_1\mu(x)$</p>
<p>Evaluation :<br>&emsp; &emsp;     $R^2$ measures the <em>goodness of fit</em> : </p>
<script type="math/tex; mode=display">R^2=\frac{SSR}{SST}=\frac{\sum(\hat y_i-\mu(y))^2}{\sum(y_i - \mu(y))^2} \in [0,1]</script><p>&emsp; &emsp;   r - correlation coefficient: </p>
<script type="math/tex; mode=display">r=corr(x,y)=\frac{covar(x,y}{std(x)std(y)}=\frac{covar(x,y)}{\sqrt{var(x)var(y)}} = \pm\sqrt{R^2}</script><h2 id="Logistic-Regression"><a href="#Logistic-Regression" class="headerlink" title="Logistic Regression"></a>Logistic Regression</h2><p>Logistic curve:</p>
<script type="math/tex; mode=display">p=\frac{e^{b_0+b_1x}}{1+e^{b_0+b_1x}}</script><script type="math/tex; mode=display">b_0+b_1x=ln\frac{p}{1-p}=ln(odds)</script><h2 id="Overfitting-and-Regularization"><a href="#Overfitting-and-Regularization" class="headerlink" title="Overfitting and Regularization"></a>Overfitting and Regularization</h2><h4 id="Overfitting"><a href="#Overfitting" class="headerlink" title="Overfitting"></a>Overfitting</h4><p>Small error on the training set but high error on test set (new examples)<br>&emsp;      Various reasons, e.g.<br>&emsp;&emsp;        • Issues with the data<br>&emsp;&emsp;&emsp;              • Noise in the training data<br>&emsp;&emsp;&emsp;              • Too small training set – does not contain enough representative examples<br>&emsp;&emsp;        • How the algorithm operates<br>&emsp;&emsp;&emsp;               • Some algorithms are more susceptible to overfitting than others<br>&emsp;&emsp;&emsp;               • Different algorithms have different strategies to deal with overfitting, e.g.<br>&emsp;&emsp;&emsp;&emsp;                • Decision tree – prune the tree<br>&emsp;&emsp;&emsp;&emsp;                     • Neural networks – early stopping of the training</p>
<h4 id="Underfitting"><a href="#Underfitting" class="headerlink" title="Underfitting"></a>Underfitting</h4><p>&emsp;• The model is too simple and doesn’t capture all important aspects of the data<br>&emsp;&emsp;       • It performs badly on both training and test data</p>
<h2 id="Ridge-and-Lasso-regression"><a href="#Ridge-and-Lasso-regression" class="headerlink" title="Ridge and Lasso regression"></a>Ridge and Lasso regression</h2><h4 id="Ridge-regression"><a href="#Ridge-regression" class="headerlink" title="Ridge regression"></a>Ridge regression</h4><p>&emsp;     Minimizes this cost function:    $\frac{1}{n} (\hat{y_i} - y_i)^2 + \alpha\sum_{i=1}^n w_i^2 $ &emsp;<br>where w close to 0, $\alpha$  controls the trade-off between performance on training set and model complexity</p>
<h4 id="Lasso-regression"><a href="#Lasso-regression" class="headerlink" title="Lasso regression"></a>Lasso regression</h4><p>LASSO = Least Absolute Shrinkage and Selection Operator Regression</p>
<p>&emsp;     Minimizes this cost function:    $\frac{1}{n} (\hat{y_i} - y_i)^2 + \alpha\sum_{i=1}^n ||w_i|| $</p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="http://example.com/2021/04/07/01006-2-Nearest-Neighbor-Algorithm/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Dong">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Dong">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2021/04/07/01006-2-Nearest-Neighbor-Algorithm/" class="post-title-link" itemprop="url">01006-2-Nearest Neighbor Algorithm.md</a>
        </h2>

        <div class="post-meta">
          
          
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>
              

              <time title="Created: 2021-04-07 10:24:22 / Modified: 18:48:43" itemprop="dateCreated datePublished" datetime="2021-04-07T10:24:22+08:00">2021-04-07</time>
            </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/01006/" itemprop="url" rel="index"><span itemprop="name">01006</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h1 id="Rule-Based-Algorithms-1R-and-PRISM"><a href="#Rule-Based-Algorithms-1R-and-PRISM" class="headerlink" title="Rule-Based Algorithms: 1R and PRISM"></a>Rule-Based Algorithms: 1R and PRISM</h1><h2 id="Nearest-Neighbor-Algorithm"><a href="#Nearest-Neighbor-Algorithm" class="headerlink" title="Nearest Neighbor Algorithm"></a>Nearest Neighbor Algorithm</h2><p>Idea:<br>&emsp;     • Remember all training examples<br>&emsp;     • To make a prediction for a new unlabelled example:<br>&emsp; &emsp;        Find the nearest training example to it using a distance measure<br>&emsp; &emsp;    The class label of the nearest neighbor will be the predicted label for the new example</p>
<h4 id="k-nearest-neighbor"><a href="#k-nearest-neighbor" class="headerlink" title="k-nearest neighbor"></a>k-nearest neighbor</h4><p>&emsp; • K-Nearest Neighbor is very sensitive to to the value of k<br>&emsp; &emsp;     • rule of thumb: $k \leq sqrt(n)$  </p>
<p>&emsp; &emsp;     • commercial packages typically use k=10<br>&emsp; • Using more nearest neighbors increases the robustness to noisy examples<br>&emsp; • K-Nearest Neighbor can be used not only for classification, but also for regression<br>&emsp; &emsp;    • The prediction will be the average value of the class values (numerical) of the k nearest neighbors</p>
<p><strong>Weighted nearest neighbor</strong>:<br>&emsp;  Idea: Closer neighbors should count more than distant neighbors</p>
<h4 id="K-nearest-neighbor-discussion"><a href="#K-nearest-neighbor-discussion" class="headerlink" title="K-nearest neighbor - discussion"></a>K-nearest neighbor - discussion</h4><p>&emsp;        • Often very accurate<br>&emsp;        • Slow for big datasets<br>&emsp;&emsp;        • requires efficient data structures such as KD-trees<br>&emsp;        • Distance-based - requires normalization<br>&emsp;        • Produces arbitrarily shaped decision boundary defined by a subset of the Voronoi edges<br>&emsp;        • Not effective for high-dimensional data (data with many features)<br>&emsp;&emsp;        • The notion of “nearness” is not effective in high dimensional data<br>&emsp;&emsp;        • Solution – dimensionality reduction and feature selection<br>&emsp;        • Sensitive to the value of k</p>
<h2 id="Rule-based-algorithms"><a href="#Rule-based-algorithms" class="headerlink" title="Rule-based algorithms"></a>Rule-based algorithms</h2><h3 id="1R-1-rule"><a href="#1R-1-rule" class="headerlink" title="1R (1-rule)"></a>1R (1-rule)</h3><p>&emsp;     1R algorithm<br>&emsp; &emsp;     • Generate a rule (decision stump) for each attribute<br>&emsp; &emsp;     • Evaluate each rule on the training data and calculate the number of errors<br>&emsp; &emsp;     • Choose the one with the smallest number of errors</p>
<p>Pseudocode</p>
<blockquote>
<p>  For each attribute,<br>&emsp; &emsp;        For each value of that attribute, make a rule as follows:<br>&emsp; &emsp; &emsp;         - count how often each class appears<br>&emsp; &emsp; &emsp;        - find the most frequent class<br>&emsp; &emsp; &emsp;        - make the rule assign that class to this attribute value.<br>&emsp; &emsp;        Calculate the error rate of the rules.<br>&emsp;      Choose the rule with the smallest error rate.</p>
</blockquote>
<h4 id="1R-discussion"><a href="#1R-discussion" class="headerlink" title="1R - discussion"></a>1R - discussion</h4><p>&emsp;   • Simple and computationally efficient algorithm<br>&emsp;   • Produces rules that are easy to understand<br>&emsp;   &emsp;   • In contrast to “black-box” models such as neural networks<br>&emsp;   • Sometimes produces surprisingly good results<br>&emsp;   • It is useful to try the simple algorithms first and compare their performance with more sophisticated algorithms<br>&emsp;   • Numerical datasets require discretization<br>&emsp;   &emsp;   • 1R has an in-built procedure to do this</p>
<h3 id="PRISM"><a href="#PRISM" class="headerlink" title="PRISM"></a>PRISM</h3><p>PRISM is an example of rule-based <em>covering</em> algorithm<br>        This type of algorithms:<br>&emsp;&emsp;        • consider each class in turn and<br>&emsp;&emsp;        • construct a set of if-then rules that cover all examples from this class and do not cover any examples from the other classes.<br>        Called covering algorithms because:<br>&emsp;&emsp;        • At each stage of the algorithm, a rule is identified that “covers” some of the examples</p>
<p><strong>PRISM algorithm</strong><br>&emsp;        Idea: Generate a rule by adding tests that maximize the rule’s accuracy<br>&emsp;        For each class:<br>&emsp;&emsp;          Start with an empty rule and gradually add conditions<br>&emsp;&emsp;           Each condition tests the value of 1 attribute<br>&emsp;&emsp;          By adding a new test, the rule coverage is reduced (i.e. the rule becomes more specific)<br>&emsp;       Finish when p/t=1 (i.e. the rule covers only examples from the same class, i.e. is “perfect”)</p>
<p>Pseudocode</p>
<blockquote>
<p>   For each class C<br>&emsp;      Initialize E to the instance set<br>&emsp;      While E contains instances in class C<br>&emsp;&emsp;        Create a rule R with an empty LHS that predicts C<br>&emsp;&emsp;        Until R is perfect do<br>&emsp;&emsp;&emsp;      For each attribute A not mentioned in R and each value V<br>&emsp;&emsp;  &emsp;&emsp;          Consider adding the condition A=v to the LHS of R<br>&emsp;&emsp;&emsp;      Select A and v to maximize p/t<br>&emsp;&emsp;&emsp;      (break ties by choosing the condition with the largest p)<br>&emsp;&emsp;&emsp;      Add A=v to R<br>&emsp;&emsp;        Remove the instances covered by R from E</p>
</blockquote>
<h4 id="PRISM-–-discussion"><a href="#PRISM-–-discussion" class="headerlink" title="PRISM – discussion"></a>PRISM – discussion</h4><p>&emsp;         • Problem – some test examples may not be covered by the PRISM rules and will not receive classification<br>&emsp; &emsp;             • Default rule is needed - assign them to the class with the most training examples<br>&emsp;         • Dealing with numeric attributes – discretization is needed</p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="http://example.com/2021/04/07/01006-1-Introduction/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Dong">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Dong">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2021/04/07/01006-1-Introduction/" class="post-title-link" itemprop="url">01006-1-Introduction.md</a>
        </h2>

        <div class="post-meta">
          
          
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>
              

              <time title="Created: 2021-04-07 10:22:32 / Modified: 17:43:59" itemprop="dateCreated datePublished" datetime="2021-04-07T10:22:32+08:00">2021-04-07</time>
            </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/01006/" itemprop="url" rel="index"><span itemprop="name">01006</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h1 id="Introduction-to-Machine-Learning-and-Data-Mining"><a href="#Introduction-to-Machine-Learning-and-Data-Mining" class="headerlink" title="Introduction to Machine Learning and Data Mining"></a>Introduction to Machine Learning and Data Mining</h1><p>DATA MINING:<br><strong>Databaes :</strong><br>&emsp; Relational data model<br>&emsp;   SQL<br>&emsp;  Association rule</p>
<p><strong>Artificial intelligence :</strong><br>&emsp;      Search algorithms</p>
<p><strong>Algorithms :</strong><br>&emsp;       Algorithm design<br>&emsp;      Algotithm analysis<br>&emsp;        Data structures</p>
<p><strong>Machine Learning :</strong><br>&emsp;      Classification and clustering algorithms<br>&emsp;    (Neural networks, decision trees, kNN, SVM, etc.)</p>
<p><strong>Statistics :</strong><br>&emsp;     Sampling, estimation, hypothesis testing<br>&emsp;     Bayes Theorem<br>&emsp;      Regression Analysis<br>&emsp;     Time Series Analysis</p>
<p><strong>Information retrieval :</strong><br>&emsp;    Similarity measures<br>&emsp;   Imprecise queries<br>&emsp;   Text/image/video data<br>&emsp;   Web search engines</p>
<h2 id="ML-and-DM-tasks"><a href="#ML-and-DM-tasks" class="headerlink" title="ML and DM tasks"></a>ML and DM tasks</h2><p>Two main types of tasks:<br>&emsp;     Supervised learning - classification and regression<br>&emsp;    Unsupervised learning - clustering<br>Others:<br>&emsp;      Association rule ming<br>&emsp;     Reinforcement learning<br>&emsp;      Outlier detection</p>
<h4 id="Supervised-learning"><a href="#Supervised-learning" class="headerlink" title="Supervised learning"></a>Supervised learning</h4><p>Given: { x , y | x - input vector, y - target output }<br>Task: learn a function that map x-&gt;y and can be used predictively<br>Types:<br>&emsp; Classification: the variable to be predicted is categorical<br>&emsp;  Regression: the variable to be predicted is numeric</p>
<h4 id="Unsupervised-learning"><a href="#Unsupervised-learning" class="headerlink" title="Unsupervised learning"></a>Unsupervised learning</h4><p>Given:{ x | x - input vectors}<br>Task: group the examples into a finite number of clusters so that $max(S_b)$ and $min(S_w)$</p>
<h2 id="The-DM-process"><a href="#The-DM-process" class="headerlink" title="The DM process"></a>The DM process</h2><ol>
<li><p>Business understanding<br> • Investigating the business objectives and requirements<br> • Deciding whether DM can be applied to meet them<br> • Determining what kind of data can be collected to build a deployable model</p>
</li>
<li><p>Data understanding<br> • Get an initial dataset; is it suitable for further processing?<br> • If the data quality is poor, collect more data based on more stringent criteria<br> • Gain insights from data and review the objective – can DM be applied?</p>
</li>
<li><p>Data preparation<br> • Cleaning: data in real world is:<br> &emsp;• Incomplete, e.g. missing values<br> &emsp;• Noisy, e.g. containing errors or outliers<br> &emsp;• Inconsistent, e.g. in codes, names<br> &emsp;Fill in missing values, smooth noisy data, identify outliers and remove them, resolve inconsistencies<br> • Transformation – convert to common format; transform to new format; perform normalization, dimensionality reduction and feature selection</p>
</li>
<li><p>Modeling<br> building ML models</p>
</li>
<li><p>Evaluation<br> • How good is the performance? E.g. accuracy, F1 measure, etc.<br> • Are the patterns meaningful and useful, or just reflecting spurious regularities?<br> • If the performance is poor, reconsider the project and return to step 1)<br> • If the performance is good -&gt; deploy it in practice</p>
</li>
<li><p>Deployment<br> • Typically requires integration into a larger software system by software engineers<br> • May be necessary to re-implement the model in a different programming language</p>
</li>
</ol>
<h2 id="Data-Cleaning"><a href="#Data-Cleaning" class="headerlink" title="Data Cleaning"></a>Data Cleaning</h2><h4 id="Noise"><a href="#Noise" class="headerlink" title="Noise"></a>Noise</h4><p>&emsp; Due to :<br>&emsp;&emsp;1. distortion of values<br>&emsp;&emsp;2. addition of spurious examples<br>&emsp;&emsp;3. inconsistent and duplicate data</p>
<p>&emsp;Reducing noise 1. and 2.:<br>&emsp;&emsp;    • Using signal and image processing and outlier detection techniques before DM<br>&emsp;&emsp;   • Using ML algorithms that are more robust to noise – give acceptable results in presence of noise</p>
<h4 id="Missing-values"><a href="#Missing-values" class="headerlink" title="Missing values"></a>Missing values</h4><p>&emsp;Methods:<br>&emsp;&emsp;1. Ignore all examples with missing values<br>&emsp;&emsp;2. Estimate the missing values by using the remaining values (average or the most common value)</p>
<h2 id="Data-preprocessing"><a href="#Data-preprocessing" class="headerlink" title="Data preprocessing"></a>Data preprocessing</h2><h4 id="Data-aggregation"><a href="#Data-aggregation" class="headerlink" title="Data aggregation"></a>Data aggregation</h4><p>&emsp;Combining two or more attributes into one<br>&emsp;purpose:<br>&emsp;&emsp;  Data reduction - less memory and computation time<br>&emsp;&emsp;  Change of scale - provides high-level view<br>&emsp;&emsp;   More stable data</p>
<h4 id="Dimensionality-reduction"><a href="#Dimensionality-reduction" class="headerlink" title="Dimensionality reduction"></a>Dimensionality reduction</h4><h4 id="Feature-extraction"><a href="#Feature-extraction" class="headerlink" title="Feature extraction"></a>Feature extraction</h4><p>&emsp;Feature extraction is the creation of features from raw data – very important task<br>&emsp;May require mapping data to a new space, then extract features</p>
<h4 id="Feature-subset-selection"><a href="#Feature-subset-selection" class="headerlink" title="Feature subset selection"></a>Feature subset selection</h4><p>&emsp;The process of removing irrelevant and redundant features and selecting a small set of features that are necessary and sufficient for good classification<br>&emsp;methods:<br>&emsp;&emsp;   Brute force (impossible)<br>&emsp;&emsp;   Embedded (decision tree)<br>&emsp;&emsp;   Filter  (feature selection)<br>&emsp;&emsp;  Wrapper </p>
<h4 id="Feature-weighting"><a href="#Feature-weighting" class="headerlink" title="Feature weighting"></a>Feature weighting</h4><p>&emsp;The more important features are assigned a higher weight, the less important – lower. This can be done manually or automatically</p>
<h4 id="Converting-attributes-from-one-type-to-another"><a href="#Converting-attributes-from-one-type-to-another" class="headerlink" title="Converting attributes from one type to another"></a>Converting attributes from one type to another</h4><p><strong>Discretization</strong>: Converting numeric attributes to nominal.<br>&emsp;Unsupervised discretization:<br>&emsp;&emsp;   1. equal width<br>&emsp;&emsp;    2. equal frequency<br>&emsp;&emsp;   3. clustering (k-means)<br>&emsp;Supervised discretization : – entropy-based<br>&emsp;&emsp;    Entropy:<br>&emsp;&emsp;     $ entropy(S)=-\sum_i P_i log_2 P_i $</p>
<p>&emsp;&emsp; $ totalEntropy = \sum_i^n w_i  entropy(S_i) $</p>
<p><strong>Binarization</strong>:  Converting numeric and nominal attributes to binary attributes.<br>&emsp;numeric attribute -&gt; categorical -&gt; integer -&gt; binary</p>
<h4 id="Normalization-and-standardization"><a href="#Normalization-and-standardization" class="headerlink" title="Normalization and standardization"></a>Normalization and standardization</h4><p>&emsp;<strong>min-max sacling</strong> : $ x`=\frac{x-min(x)}{max(x)-min(x)} $</p>
<p>&emsp;standardization:  $ x`= \frac{x-\mu(x)}{\theta(x)} $</p>
<h2 id="Similarity-measures"><a href="#Similarity-measures" class="headerlink" title="Similarity measures"></a>Similarity measures</h2><p>two measures: distance / correlation</p>
<h4 id="Distance"><a href="#Distance" class="headerlink" title="Distance"></a>Distance</h4><p>For numeric attributes:<br>&emsp;    <strong>Minkowski distance</strong>:<br>&emsp; &emsp; $ D(A,B) = { (|a_1-b_1|^q + |a_2-b_2|^q + … + |a_n-b_n|^q ) }^{1/q} $</p>
<p>&emsp;      q=1: Manhattan distance (L1 norm)<br>&emsp;      q=2: Euclidean distance (L2 norm)<br>For binary vectors:<br>&emsp;   Simple Matching Coefficient (<strong>SMC</strong>) - matching 1-1 and 0-0 / num. attributes<br>&emsp;&emsp;   $ SMC=\frac{f11+f00}{f01+f10+f11+f00} $   </p>
<p>&emsp;SMC is not suitable for sparse data, therefore has an alternative: </p>
<p>   &emsp;   <strong>Jaccard coefficient</strong>:<br>   &emsp; &emsp;       $ J = \frac{f11}{f01+f10+f11} $</p>
<p>   &emsp;   <strong>Cosine similarity</strong>: useful for sparse data<br>   &emsp; &emsp;    $ cos(A, B)= \frac{A \dot B}{||A|| ||B||} $</p>
<h4 id="Correlation"><a href="#Correlation" class="headerlink" title="Correlation"></a>Correlation</h4><p>&emsp;<strong>Pearson correlation coefficient</strong> :<br>&emsp;&emsp; $ corr(x, y) = \frac{covar(x, y)}{std(x)std(y)} $,</p>
<p>where: $mean(x) = \frac{\sum_{i=1}^n x_i}{n} $,<br>$std(x) = \sqrt{\frac{\sum_{i=1}^(x_i-mean(x))^2 }{n-1}}$,<br>$covar(x, y) = \frac{1}{n-1} \sum_{i=1}^n(x_i-mean(x))(y_i-mean(y))$</p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="http://example.com/2020/12/16/01005-3-BigData/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Dong">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Dong">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2020/12/16/01005-3-BigData/" class="post-title-link" itemprop="url">01005-3:BigData</a>
        </h2>

        <div class="post-meta">
          
          
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2020-12-16 19:16:23" itemprop="dateCreated datePublished" datetime="2020-12-16T19:16:23+08:00">2020-12-16</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2021-01-10 17:36:03" itemprop="dateModified" datetime="2021-01-10T17:36:03+08:00">2021-01-10</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/01005/" itemprop="url" rel="index"><span itemprop="name">01005</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h2 id="Hadoop项目结构"><a href="#Hadoop项目结构" class="headerlink" title="Hadoop项目结构"></a>Hadoop项目结构</h2><h2 id="Hadoop集群部署"><a href="#Hadoop集群部署" class="headerlink" title="Hadoop集群部署"></a>Hadoop集群部署</h2><h3 id="HDFS"><a href="#HDFS" class="headerlink" title="HDFS"></a>HDFS</h3><ul>
<li>NameNode  直接存储到内存<ul>
<li>FsImage <ul>
<li>文件的复制等级、修改和访问时间、访问权限、块大小以及组成文件的块</li>
</ul>
</li>
<li>EditLog<ul>
<li>各项操作日志</li>
</ul>
</li>
</ul>
</li>
<li>DataNode</li>
<li>SecondaryNameNode 作为冷备份<br>解决EditLog不断增大的问题</li>
</ul>
<h4 id="实现目标"><a href="#实现目标" class="headerlink" title="实现目标"></a>实现目标</h4><ul>
<li>兼容廉价的硬件设备</li>
<li>实现流数据读写</li>
<li>支持大数据集</li>
<li>支持简单的文件模型</li>
<li>强大的跨平台兼容性</li>
</ul>
<h4 id="自身局限"><a href="#自身局限" class="headerlink" title="自身局限"></a>自身局限</h4><ul>
<li>不适合低延迟数据访问（实时性处理需求）</li>
<li>无法高效存储大量小文件</li>
<li>不支持多用户写入及任意修改文件</li>
</ul>
<h3 id="存储原理"><a href="#存储原理" class="headerlink" title="存储原理"></a>存储原理</h3><ul>
<li>冗余存储（一般三个副本）<ul>
<li>加快数据传输速度</li>
<li>很容易检查数据错误</li>
<li>保证数据可靠性</li>
</ul>
</li>
</ul>
<h3 id="MapReduce"><a href="#MapReduce" class="headerlink" title="MapReduce"></a>MapReduce</h3><p>JobTracker<br>TaskTracker</p>
<h2 id="HBase"><a href="#HBase" class="headerlink" title="HBase"></a>HBase</h2><h3 id="简介"><a href="#简介" class="headerlink" title="简介"></a>简介</h3><h4 id="HBase-和关系型数据库的区别"><a href="#HBase-和关系型数据库的区别" class="headerlink" title="HBase 和关系型数据库的区别"></a>HBase 和关系型数据库的区别</h4><p>数据类型    RM          字符串<br>数据操作    多            字符串<br>存储模式    基于行     基于列<br>数据索引    复杂<br>数据维护    覆盖旧值    保留<br>可伸缩性    纵向扩展    横向扩展</p>
<h3 id="数据模型"><a href="#数据模型" class="headerlink" title="数据模型"></a>数据模型</h3><p>行键<br>列族<br>    支持动态扩展<br>    保留旧的版本<br>列限定符<br>单元格<br>时间戳</p>
<p>列式存储：数据类型相同，高数据压缩率  —分析型操作<br>行式存储：数据类型不同，低数据压缩率  —事务型操作</p>
<h3 id="实现原理"><a href="#实现原理" class="headerlink" title="实现原理"></a>实现原理</h3><h4 id="功能组件"><a href="#功能组件" class="headerlink" title="功能组件"></a>功能组件</h4><p>库函数<br>Master服务器<br>    增删改查、负载均衡、分裂合并、重新分配<br>Region服务器</p>
<h4 id="三层结构"><a href="#三层结构" class="headerlink" title="三层结构"></a>三层结构</h4><p>ZooKeeper 文件：记录ROOT表的位置<br>ROOT表 ：记录META表中的Region位置<br>META表 ：记录用户数据表的Region位置</p>
<h3 id="运行机制"><a href="#运行机制" class="headerlink" title="运行机制"></a>运行机制</h3><p>写：缓存 - MemStore - 写日志(HLog)<br>读：缓存 - MemStore - StroeFile</p>
<h3 id="应用方案"><a href="#应用方案" class="headerlink" title="应用方案"></a>应用方案</h3><p>性能优化方法<br>    时间戳：Long.MAX_VALUE - timestamp作为行键<br>    读写性能：表放到Region缓存里<br>    存储：设置最大版本数<br>                TimeToLive<br>检测性能：<br>    Master-status<br>    Ganglia<br>    OpenTSDB<br>    Ambari</p>
<p>SQL:<br>    Hive<br>    Phoenix</p>
<h2 id="NoSQL"><a href="#NoSQL" class="headerlink" title="NoSQL"></a>NoSQL</h2><h3 id="概述"><a href="#概述" class="headerlink" title="概述"></a>概述</h3><h4 id="特点"><a href="#特点" class="headerlink" title="特点"></a>特点</h4><p>灵活的可扩展性<br>灵活的数据模型<br>和云计算的结合</p>
<h3 id="与RDBMS的比较"><a href="#与RDBMS的比较" class="headerlink" title="与RDBMS的比较"></a>与RDBMS的比较</h3><p>关系型数据库  NoSQL<br>数据库原理       有完备的代数理论    缺乏<br>数据规模           难以横向扩展      高水平可扩展性<br>数据库模式       严格遵守            模型灵活<br>查询效率           适当量级效率高 复杂查询效率差<br>事务一致性       ACID事务模型    不能保持强<br>数据完整性       完备机制            不能<br>可扩展性           差       好<br>可用性               规模增大可用性下降  高可用性<br>标准化               完善    未形成行业标准<br>技术支持            强大      发展初期<br>可维护                 管理员维护       较为复杂</p>
<h3 id="四大类型"><a href="#四大类型" class="headerlink" title="四大类型"></a>四大类型</h3><ul>
<li><p>键值数据库<br>redis<br>频繁读写<br>理想的缓冲层解决方案</p>
</li>
<li><p>列族数据库<br>hbase</p>
</li>
<li>图数据库<br>Neo4j<br>高度互相关联关系</li>
<li>文档数据库<br>mongoDB<br>json格式</li>
</ul>
<h3 id="理论基石"><a href="#理论基石" class="headerlink" title="理论基石"></a>理论基石</h3><ul>
<li><p>CAP<br>Consistency一致性<br>Availablity 可用性<br>Partition toleration 分区容忍性<br>CA：MySQL<br>CP：MongoDB, Hbase, Redis, Neo4j<br>AP：Cassandra</p>
</li>
<li><p>BASE<br>Basically Availble Soft-state &amp; Eventual consistency<br>基本可用、软状态、最终一致性</p>
</li>
<li><p>最终一致性<br>  因果一致性<br>  “读己之所写”一致性<br>  单调读一致性<br>  会话一致性<br>  单调写一致性</p>
</li>
</ul>
<p>N：冗余份数<br>W：已写完的节点数<br>R：需要读取的节点数<br>W+R&gt;N: 强一致性<br>W+R&lt;=N: 弱一致性</p>
<h3 id="从NoSQL到NewSQL"><a href="#从NoSQL到NewSQL" class="headerlink" title="从NoSQL到NewSQL"></a>从NoSQL到NewSQL</h3><p>分析型：NewSQL<br>事务型：OldSQL<br>互联网型：NoSQL </p>
<h2 id="云数据库"><a href="#云数据库" class="headerlink" title="云数据库"></a>云数据库</h2><h3 id="概述-1"><a href="#概述-1" class="headerlink" title="概述"></a>概述</h3><ul>
<li>云计算优势：<br>按需服务、随时服务、通用性、高可靠性、极其廉价、超大规模、</li>
<li>云数据库优良特性：<br>动态可扩展、较低的使用代价、易用性、高可用性、免维护、高性能、安全</li>
</ul>
<h3 id="UMP系统"><a href="#UMP系统" class="headerlink" title="UMP系统"></a>UMP系统</h3><ul>
<li>原则：</li>
</ul>
<ol>
<li>单一对外访问入口</li>
<li>消除单点故障</li>
<li>良好的可伸缩性</li>
<li>资源的相互隔离</li>
</ol>
<ul>
<li><p>系统架构：<br>Mnesia：分布式数据管理系统，支持事务<br>RabbitMQ：消息队列<br>ZooKeeper：高效的协调服务：全局配置服务器、提供分布式锁、监控MySQL实例<br>LVS：实现集群内部的负载均衡<br>Controller服务器：集群成员管理、元数据存储、MySQL实例管理、故障恢复、备份迁移扩容<br>Web管理台：提供系统管理界面<br>Proxy服务器：向用户提供访问MySQL的服务<br>Agent服务器：管理物理机上的MySQL实例<br>日志分析服务器：对整个日志分析<br>信息统计服务器：用户连接数、QPS、MySQL实例进程状态<br>愚公系统：数据迁移</p>
</li>
<li><p>系统功能：<br>容灾：主库、从库<br>读写分离：写-主，读-主/从<br>分库分表：proxy解析MySQL，分发到对应的实例上，接收执行结果<br>资源管理：资源池机制进行管理</p>
</li>
</ul>
<h3 id="AWS"><a href="#AWS" class="headerlink" title="AWS"></a>AWS</h3><h3 id="SQL-Azure"><a href="#SQL-Azure" class="headerlink" title="SQL Azure"></a>SQL Azure</h3><h2 id="MapReduce-1"><a href="#MapReduce-1" class="headerlink" title="MapReduce"></a>MapReduce</h2><h3 id="模型简介"><a href="#模型简介" class="headerlink" title="模型简介"></a>模型简介</h3><p>计算向数据靠拢</p>
<h3 id="体系结构"><a href="#体系结构" class="headerlink" title="体系结构"></a>体系结构</h3><p>Client<br>JobTracker：资源监控、作业调度<br>TaskSheduler：<br>TaskTracker ：执行具体任务</p>
<h3 id="工作流程："><a href="#工作流程：" class="headerlink" title="工作流程："></a>工作流程：</h3><p>InputFormat<br>Split<br>RR(Record Read)<br>Map<br>Shuffle<br>Reduce<br>OutputFormat</p>
<h3 id="Shuffle"><a href="#Shuffle" class="headerlink" title="Shuffle"></a>Shuffle</h3><ul>
<li>Map Shuffle:</li>
</ul>
<ol>
<li>输入数据和执行Map任务</li>
<li>写入缓存</li>
<li>溢写（分区、排序、合并）</li>
<li>文件归并</li>
</ol>
<ul>
<li>Reduce Shuffle:</li>
</ul>
<ol>
<li>领取数据</li>
<li>归并数据</li>
<li>把数据输入给Reduce任务</li>
</ol>
<h3 id="执行过程"><a href="#执行过程" class="headerlink" title="执行过程"></a>执行过程</h3><p>程序部署<br>分配Map任务<br>读数据<br>本地写数据<br>远程读数据<br>写数据</p>
<h2 id="Hadoop-再探讨"><a href="#Hadoop-再探讨" class="headerlink" title="Hadoop 再探讨"></a>Hadoop 再探讨</h2><h3 id="Hadoop-2-0"><a href="#Hadoop-2-0" class="headerlink" title="Hadoop 2.0"></a>Hadoop 2.0</h3><h4 id="Hadoop-HA-High-Availablity"><a href="#Hadoop-HA-High-Availablity" class="headerlink" title="Hadoop HA (High Availablity)"></a>Hadoop HA (High Availablity)</h4><p>Zookeeper: 管理active / standby (热备份) NameNode</p>
<h4 id="Hadoop-Federation"><a href="#Hadoop-Federation" class="headerlink" title="Hadoop Federation"></a>Hadoop Federation</h4><p>解决HDFS集群扩展性问题、性能更高效、良好的隔离性</p>
<h3 id="YARN"><a href="#YARN" class="headerlink" title="YARN"></a>YARN</h3><h4 id="MapReduce-1-0"><a href="#MapReduce-1-0" class="headerlink" title="MapReduce 1.0"></a>MapReduce 1.0</h4><p>缺陷：单点故障、JobTracker 任务过重、容易内存溢出、资源化分不合理</p>
<h4 id="设计思路"><a href="#设计思路" class="headerlink" title="设计思路"></a>设计思路</h4><p>ResourceManager: 资源管理<br>ApplicationManager: 任务调度、任务监控<br>NodeManager: TaskTracker</p>
<h4 id="体系结构-1"><a href="#体系结构-1" class="headerlink" title="体系结构"></a>体系结构</h4><p>ResourceManager：处理客户端请求、启动/监控ApplicationManager、监控NodeManager、资源分配与调度<br>ApplicationManager：为应用程序申请资源，分配任务、任务调度、监控与容错<br>NodeManager: 单个节点的资源管理、处理命令</p>
<h4 id="工作流程"><a href="#工作流程" class="headerlink" title="工作流程"></a>工作流程</h4><ol>
<li>向yarn提交应用程序；</li>
<li>ResourceManager接受处理、分配容器并启动ApplicationManager；</li>
<li>ApplicationManager创建后向ResourceManager注册；</li>
<li>ApplicationManager轮询 向ResourceManager申请资源；</li>
<li>ResourceManager以容器的形式对ApplicationManager分配资源；</li>
<li>在容器中启动任务（分配Map/Reduce 任务）；</li>
<li>各个任务向ApplicationManager汇报状态和进度；</li>
<li>运行完成后，ApplicationManager向ResourceManager的应用程序管理器注销并关闭自己。</li>
</ol>
<h3 id="Hadoop生态系统组件"><a href="#Hadoop生态系统组件" class="headerlink" title="Hadoop生态系统组件"></a>Hadoop生态系统组件</h3><h4 id="Pig"><a href="#Pig" class="headerlink" title="Pig"></a>Pig</h4><p>Pig Latin 类似SQL</p>
<h4 id="Tez"><a href="#Tez" class="headerlink" title="Tez"></a>Tez</h4><p>支持DAG作业的计算框架<br>可去除工作流中多余的Map阶段、减少写入HDFS过程</p>
<h4 id="Spark"><a href="#Spark" class="headerlink" title="Spark"></a>Spark</h4><p>可应用于大规模数据处理的快速通用引擎</p>
<h4 id="Kafka"><a href="#Kafka" class="headerlink" title="Kafka"></a>Kafka</h4><p>作为数据交换中枢<br>高吞吐量的分布式发布订阅消息，可满足在线实时处理和批量离线处理</p>
<h2 id="Hive"><a href="#Hive" class="headerlink" title="Hive"></a>Hive</h2><h3 id="数据仓库"><a href="#数据仓库" class="headerlink" title="数据仓库"></a>数据仓库</h3><p>面向主题的、集成的、相对稳定的、反映历史变化的数据集合，用于支持管理决策</p>
<h3 id="Hive-1"><a href="#Hive-1" class="headerlink" title="Hive"></a>Hive</h3><p>不支持数据存储和处理</p>
<ol>
<li>采用批处理方式处理海量数据</li>
<li>提供了一系例对数据进行ETL的工具<h4 id="系统架构"><a href="#系统架构" class="headerlink" title="系统架构"></a>系统架构</h4></li>
</ol>
<ul>
<li>对外访问接口<br>CLI<br>HWI<br>JDBC ODBC<br>Thrift Server</li>
<li>驱动模块<br>编译器、优化器、执行器</li>
<li>元数据存储模块<br>独立的关系型数据库</li>
</ul>
<h4 id="Hive-HA基本原理"><a href="#Hive-HA基本原理" class="headerlink" title="Hive HA基本原理"></a>Hive HA基本原理</h4><p>设置多个Hive实例构成一个资源池<br>HA Proxy -&gt; 逻辑可用性测试 -&gt; HA Proxy  对列入黑名单的实例进行统一处理</p>
<h3 id="SQL转换成MapReduce作业原理"><a href="#SQL转换成MapReduce作业原理" class="headerlink" title="SQL转换成MapReduce作业原理"></a>SQL转换成MapReduce作业原理</h3><ul>
<li>join操作</li>
</ul>
<ol>
<li>编写Map处理逻辑</li>
<li>Map处理逻辑输入关系数据库的表</li>
<li>通过Map对他进行转换</li>
</ol>
<h4 id="原理"><a href="#原理" class="headerlink" title="原理"></a>原理</h4><p>输入 -&gt; SQL语句 -&gt; 抽象语法树 -&gt; 查询块 -&gt; 逻辑查询计划 -&gt;  优化合并、重写逻辑查询计划 -&gt; MapReduce   -&gt; 输出</p>
<h3 id="Impala"><a href="#Impala" class="headerlink" title="Impala"></a>Impala</h3><h4 id="系统架构-1"><a href="#系统架构-1" class="headerlink" title="系统架构"></a>系统架构</h4><p> Impalad：负责具体的相关查询任务</p>
<pre><code>- Query Planner + Query Coordinator + Query Exec Engine 
- 给其他Impalad分配任务及执行结果汇总
</code></pre><p> State Store：元数据管理和状态信息维护</p>
<pre><code>- 收集分布在各个集群中的Impalad进程的资源信息用于查询调度
</code></pre><p> CLI：用户访问接口 </p>
<h4 id="查询执行过程"><a href="#查询执行过程" class="headerlink" title="查询执行过程"></a>查询执行过程</h4><ol>
<li>注册和订阅Impalad进程</li>
<li>提交查询</li>
<li>获取元数据和数据地址信息</li>
<li>分发查询任务</li>
<li>汇聚结果</li>
<li>返回结果</li>
</ol>
<h4 id="Hive-VS-Impala"><a href="#Hive-VS-Impala" class="headerlink" title="Hive VS Impala"></a>Hive VS Impala</h4><p>Hive： 长时间的批处理查询分析、内存放不下会使用外存<br>Impala：实时交互式SQL查询（不经过MapReduce）、不会利用外存</p>
<h2 id="Spark-1"><a href="#Spark-1" class="headerlink" title="Spark"></a>Spark</h2><h2 id="流计算"><a href="#流计算" class="headerlink" title="流计算"></a>流计算</h2><h2 id="Flink"><a href="#Flink" class="headerlink" title="Flink"></a>Flink</h2><h2 id="图计算"><a href="#图计算" class="headerlink" title="图计算"></a>图计算</h2>
      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="http://example.com/2020/12/16/01005-2-Data-Mining/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Dong">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Dong">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2020/12/16/01005-2-Data-Mining/" class="post-title-link" itemprop="url">01005-2:Data Mining</a>
        </h2>

        <div class="post-meta">
          
          
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>
              

              <time title="Created: 2020-12-16 19:14:52 / Modified: 19:16:39" itemprop="dateCreated datePublished" datetime="2020-12-16T19:14:52+08:00">2020-12-16</time>
            </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/01005/" itemprop="url" rel="index"><span itemprop="name">01005</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          
      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="http://example.com/2020/11/18/01004-9-Review/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Dong">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Dong">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2020/11/18/01004-9-Review/" class="post-title-link" itemprop="url">01004-9:Review</a>
        </h2>

        <div class="post-meta">
          
          
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2020-11-18 14:17:28" itemprop="dateCreated datePublished" datetime="2020-11-18T14:17:28+08:00">2020-11-18</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2020-11-13 14:34:17" itemprop="dateModified" datetime="2020-11-13T14:34:17+08:00">2020-11-13</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/01004/" itemprop="url" rel="index"><span itemprop="name">01004</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          
      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="http://example.com/2020/11/17/01002-11-Review/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Dong">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Dong">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2020/11/17/01002-11-Review/" class="post-title-link" itemprop="url">01002-11:Review</a>
        </h2>

        <div class="post-meta">
          
          
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2020-11-17 11:27:38" itemprop="dateCreated datePublished" datetime="2020-11-17T11:27:38+08:00">2020-11-17</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2020-12-04 16:41:11" itemprop="dateModified" datetime="2020-12-04T16:41:11+08:00">2020-12-04</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/01002/" itemprop="url" rel="index"><span itemprop="name">01002</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h1 id="Review"><a href="#Review" class="headerlink" title="Review"></a>Review</h1><h2 id="Kinds-of-Data-and-Exploratory-Analysis"><a href="#Kinds-of-Data-and-Exploratory-Analysis" class="headerlink" title="Kinds of Data and Exploratory Analysis"></a>Kinds of Data and Exploratory Analysis</h2><h3 id="Kinds-of-Data"><a href="#Kinds-of-Data" class="headerlink" title="Kinds of Data"></a>Kinds of Data</h3><ul>
<li>Data(File) Formats:<ul>
<li>Structrured; Semi-Structured; Unstructured;</li>
</ul>
</li>
<li>Kinds of Data:<ul>
<li>Nominal; Oridinal; Interval; Ratio; </li>
</ul>
</li>
</ul>
<h3 id="Descriptive-Statistics"><a href="#Descriptive-Statistics" class="headerlink" title="Descriptive Statistics"></a>Descriptive Statistics</h3><ul>
<li>Measures of central tendency:<br>  -mean, median, mode</li>
<li>Measures of dispersion (or spread):<ul>
<li>count, min, max, range, percentiles, variance, stddeviation, IQR, percentiles</li>
</ul>
</li>
</ul>
<h3 id="Data-Visualisation"><a href="#Data-Visualisation" class="headerlink" title="Data Visualisation"></a>Data Visualisation</h3><ul>
<li>Nominal:  histograms, bar chart</li>
<li>Oridinal: bar charts, boxplots, histograms</li>
<li>Interval: scatter plot, binned histograms, frequency polygons</li>
</ul>
<h2 id="Clustering"><a href="#Clustering" class="headerlink" title="Clustering"></a>Clustering</h2><h3 id="Data-Clustering"><a href="#Data-Clustering" class="headerlink" title="Data Clustering"></a>Data Clustering</h3><p>Types:</p>
<h4 id="Partitional-clustering"><a href="#Partitional-clustering" class="headerlink" title="Partitional clustering:"></a>Partitional clustering:</h4><p>K-means:<br>Using Sihouette to choose k<br><strong>important</strong></p>
<h4 id="Hierarchical-clustering"><a href="#Hierarchical-clustering" class="headerlink" title="Hierarchical clustering:"></a>Hierarchical clustering:</h4><h2 id="Association-Rule"><a href="#Association-Rule" class="headerlink" title="Association Rule"></a>Association Rule</h2><ul>
<li>Support count</li>
<li>Support</li>
<li>Frequent itemset</li>
<li>Confidence<br><strong>important</strong><h3 id="Mining-Association-Rules"><a href="#Mining-Association-Rules" class="headerlink" title="Mining Association Rules"></a>Mining Association Rules</h3>FP-growth<br>Apriori</li>
</ul>
<h2 id="Data-Processing-and-Management"><a href="#Data-Processing-and-Management" class="headerlink" title="Data Processing and Management"></a>Data Processing and Management</h2><p>SQL</p>
<h2 id="Statistical-Inference"><a href="#Statistical-Inference" class="headerlink" title="Statistical Inference"></a>Statistical Inference</h2><h3 id="Research-question"><a href="#Research-question" class="headerlink" title="Research question"></a>Research question</h3><ul>
<li>Research question(Q)</li>
<li>Null hypothesis(H0)</li>
</ul>
<h3 id="Testing-reliability-with-p-values"><a href="#Testing-reliability-with-p-values" class="headerlink" title="Testing reliability with p-values"></a>Testing reliability with p-values</h3><h3 id="Measurement"><a href="#Measurement" class="headerlink" title="Measurement"></a>Measurement</h3><ul>
<li>Accuracy</li>
<li>Precision</li>
<li>Recall</li>
<li>F1</li>
</ul>
<h2 id="Machine-Learning"><a href="#Machine-Learning" class="headerlink" title="Machine Learning"></a>Machine Learning</h2><h3 id="how-to-learn-theta"><a href="#how-to-learn-theta" class="headerlink" title="how to learn $\theta$"></a>how to learn $\theta$</h3><h3 id="gradient-descent"><a href="#gradient-descent" class="headerlink" title="gradient descent"></a>gradient descent</h3><h3 id="prevent-overfitting"><a href="#prevent-overfitting" class="headerlink" title="prevent overfitting"></a>prevent overfitting</h3><p>regularisation</p>
<h3 id="Information-Gain"><a href="#Information-Gain" class="headerlink" title="Information Gain"></a>Information Gain</h3><h3 id="Decision-tress"><a href="#Decision-tress" class="headerlink" title="Decision tress"></a>Decision tress</h3><p><strong>important</strong></p>
<h3 id="Naive-Bayes-Classifier"><a href="#Naive-Bayes-Classifier" class="headerlink" title="Naive Bayes Classifier"></a>Naive Bayes Classifier</h3><p><strong>important</strong></p>
<h2 id="Q"><a href="#Q" class="headerlink" title="Q:"></a>Q:</h2><p>W5:<br>Stu􏰉􏰎dent`s t-test<br>Mann-Whitney U test<br>Analysis of variance (ANOVA)<br>Kruskall-Wallis H-test</p>
<p>W10:<br>ID3</p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  


  
  <nav class="pagination">
    <span class="page-number current">1</span><a class="page-number" href="/page/2/">2</a><span class="space">&hellip;</span><a class="page-number" href="/page/6/">6</a><a class="extend next" rel="next" href="/page/2/"><i class="fa fa-angle-right" aria-label="Next page"></i></a>
  </nav>



          </div>
          

<script>
  window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          Table of Contents
        </li>
        <li class="sidebar-nav-overview">
          Overview
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
  <p class="site-author-name" itemprop="name">Dong</p>
  <div class="site-description" itemprop="description"></div>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives/">
        
          <span class="site-state-item-count">55</span>
          <span class="site-state-item-name">posts</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
            <a href="/categories/">
          
        <span class="site-state-item-count">6</span>
        <span class="site-state-item-name">categories</span></a>
      </div>
      <div class="site-state-item site-state-tags">
            <a href="/tags/">
          
        <span class="site-state-item-count">12</span>
        <span class="site-state-item-name">tags</span></a>
      </div>
  </nav>
</div>
  <div class="links-of-author motion-element">
      <span class="links-of-author-item">
        <a href="https://github.com/liddalidd" title="GitHub → https:&#x2F;&#x2F;github.com&#x2F;liddalidd" rel="noopener" target="_blank"><i class="fab fa-github fa-fw"></i>GitHub</a>
      </span>
      <span class="links-of-author-item">
        <a href="mailto:liddaalidd@163.com" title="E-Mail → mailto:liddaalidd@163.com" rel="noopener" target="_blank"><i class="fa fa-envelope fa-fw"></i>E-Mail</a>
      </span>
  </div>



      </div>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        

        

<div class="copyright">
  
  &copy; 
  <span itemprop="copyrightYear">2021</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Dong</span>
</div>
  <div class="powered-by">Powered by <a href="https://hexo.io/" class="theme-link" rel="noopener" target="_blank">Hexo</a> & <a href="https://theme-next.org/" class="theme-link" rel="noopener" target="_blank">NexT.Gemini</a>
  </div>

        
<div class="busuanzi-count">
  <script async src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>
    <span class="post-meta-item" id="busuanzi_container_site_uv" style="display: none;">
      <span class="post-meta-item-icon">
        <i class="fa fa-user"></i>
      </span>
      <span class="site-uv" title="Total Visitors">
        <span id="busuanzi_value_site_uv"></span>
      </span>
    </span>
    <span class="post-meta-divider">|</span>
    <span class="post-meta-item" id="busuanzi_container_site_pv" style="display: none;">
      <span class="post-meta-item-icon">
        <i class="fa fa-eye"></i>
      </span>
      <span class="site-pv" title="Total Views">
        <span id="busuanzi_value_site_pv"></span>
      </span>
    </span>
</div>








      </div>
    </footer>
  </div>

  
  <script src="/lib/anime.min.js"></script>
  <script src="/lib/velocity/velocity.min.js"></script>
  <script src="/lib/velocity/velocity.ui.min.js"></script>

<script src="/js/utils.js"></script>

<script src="/js/motion.js"></script>


<script src="/js/schemes/pisces.js"></script>


<script src="/js/next-boot.js"></script>




  















  

  
      

<script>
  if (typeof MathJax === 'undefined') {
    window.MathJax = {
      loader: {
        source: {
          '[tex]/amsCd': '[tex]/amscd',
          '[tex]/AMScd': '[tex]/amscd'
        }
      },
      tex: {
        inlineMath: {'[+]': [['$', '$']]},
        tags: 'ams'
      },
      options: {
        renderActions: {
          findScript: [10, doc => {
            document.querySelectorAll('script[type^="math/tex"]').forEach(node => {
              const display = !!node.type.match(/; *mode=display/);
              const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display);
              const text = document.createTextNode('');
              node.parentNode.replaceChild(text, node);
              math.start = {node: text, delim: '', n: 0};
              math.end = {node: text, delim: '', n: 0};
              doc.math.push(math);
            });
          }, '', false],
          insertedScript: [200, () => {
            document.querySelectorAll('mjx-container').forEach(node => {
              let target = node.parentNode;
              if (target.nodeName.toLowerCase() === 'li') {
                target.parentNode.classList.add('has-jax');
              }
            });
          }, '', false]
        }
      }
    };
    (function () {
      var script = document.createElement('script');
      script.src = '//cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js';
      script.defer = true;
      document.head.appendChild(script);
    })();
  } else {
    MathJax.startup.document.state(0);
    MathJax.texReset();
    MathJax.typeset();
  }
</script>

    

  

</body>
</html>
